{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety Helmet Detection\n",
    "\n",
    "Welcome to this SageMaker Notebook! This is an entirely managed notebook service that you can use to create and edit machine learning models. We will be using it today to create a binary image classification model using the Apache MXNet deep learning framework. We will then learn how to deploy this model onto our DeepLens device.\n",
    "\n",
    "In this notebook we will be to using MXNet's Gluon interface, to download and edit a pre-trained ImageNet model and transform it into binary classifier, which we can use to differentiate between hot dogs and not hot dogs.\n",
    "\n",
    "### Setup\n",
    "\n",
    "Before we start, make sure the kernel in the the notebook is set to the correct one, `condamxnet3.6` which has all the dependencies we will need for this tutorial already installed.\n",
    "\n",
    "First we'll start by importing a bunch of packages into the notebook that you'll need later and installing any required packages that are missing into our notebook kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/mxnet_p36\n",
      "\n",
      "  added / updated specs: \n",
      "    - scikit-image\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2018.8.13          |           py36_0         138 KB\n",
      "    scikit-image-0.14.0        |   py36hf484d3e_1        24.1 MB\n",
      "    openssl-1.0.2p             |       h14c3975_0         3.5 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        27.7 MB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    certifi:         2018.4.16-py36_0      conda-forge --> 2018.8.13-py36_0     \n",
      "    openssl:         1.0.2o-0              conda-forge --> 1.0.2p-h14c3975_0    \n",
      "    scikit-image:    0.13.1-py36h14c3975_1             --> 0.14.0-py36hf484d3e_1\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "    ca-certificates: 2018.4.16-0           conda-forge --> 2018.03.07-0         \n",
      "\n",
      "Proceed ([y]/n)? \n",
      "\n",
      "Downloading and Extracting Packages\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.4.10\n",
      "  latest version: 4.5.10\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\r",
      "certifi 2018.8.13:            |   0% \r",
      "certifi 2018.8.13: ########## | 100% \n",
      "\r",
      "scikit-image 0.14.0:            |   0% \r",
      "scikit-image 0.14.0: #5         |  16% \r",
      "scikit-image 0.14.0: ###3       |  33% \r",
      "scikit-image 0.14.0: #####1     |  51% \r",
      "scikit-image 0.14.0: ######8    |  69% \r",
      "scikit-image 0.14.0: ########1  |  82% \r",
      "scikit-image 0.14.0: #########2 |  92% \r",
      "scikit-image 0.14.0: ########## | 100% \n",
      "\r",
      "openssl 1.0.2p:            |   0% \r",
      "openssl 1.0.2p: #######5   |  75% \r",
      "openssl 1.0.2p: ########2  |  83% \r",
      "openssl 1.0.2p: #########5 |  96% \r",
      "openssl 1.0.2p: ########## | 100% \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "conda install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import os\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import skimage.io as io\n",
    "import numpy as np\n",
    "\n",
    "from mxnet import gluon, image, init, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "\n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The model we will be downloading and editing is [SqueezeNet](https://arxiv.org/abs/1602.07360), an extremely efficient image classification model that achived 2012 State of the Art accuracy on the popular [ImageNet](http://www.image-net.org/challenges/LSVRC/), image classification challenge. SqueezeNet is just a convolutional neural network, with an architecture chosen to have a small number of parameters and to require a minimal amount of computation. It's especially popular for folks that need to run CNNs on low-powered devices like cell phones and other internet-of-things devices, such as DeepLens. The MXNet Deep Learning framework offers squeezenet v1.0 and v1.1 that are pretrained on ImageNet through it's model Zoo.\n",
    "\n",
    "## Pulling the pre-trained model\n",
    "The MXNet model zoo  gives us convenient access to a number of popular models,\n",
    "both their architectures and their pretrained parameters.\n",
    "Let's download SqueezeNet right now with just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo mode uses the validation dataset for training, which is smaller and faster to train.\n",
    "demo = True\n",
    "log_interval = 100\n",
    "gpus = 0\n",
    "\n",
    "# Options are imperative or hybrid. Use hybrid for better performance.\n",
    "mode = 'hybrid'\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 256\n",
    "if demo:\n",
    "    epochs = 5\n",
    "    learning_rate = 0.02\n",
    "    wd = 0.002\n",
    "else:\n",
    "    epochs = 40\n",
    "    learning_rate = 0.05\n",
    "    wd = 0.002\n",
    "\n",
    "# the class weight for hotdog class to help the imbalance problem.\n",
    "positive_class_weight = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import os\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import skimage.io as io\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet.test_utils import download\n",
    "mx.random.seed(127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 23\n",
    "\n",
    "epochs = 5\n",
    "lr = 0.001\n",
    "per_device_batch_size = 1\n",
    "momentum = 0.9\n",
    "wd = 0.0001\n",
    "\n",
    "lr_factor = 0.75\n",
    "lr_steps = [10, 20, 30, np.inf]\n",
    "\n",
    "num_gpus = 0\n",
    "num_workers = 1\n",
    "ctx = [mx.gpu(i) for i in range(num_gpus)] if num_gpus > 0 else [mx.cpu()]\n",
    "batch_size = per_device_batch_size * max(num_gpus, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitter_param = 0.4\n",
    "lighting_param = 0.1\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomFlipLeftRight(),\n",
    "    transforms.RandomColorJitter(brightness=jitter_param, contrast=jitter_param,\n",
    "                                 saturation=jitter_param),\n",
    "    transforms.RandomLighting(lighting_param),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './'\n",
    "train_path = os.path.join(path, 'train')\n",
    "#val_path = os.path.join(path, 'val')\n",
    "test_path = os.path.join(path, 'test')\n",
    "\n",
    "train_dataset = gluon.data.DataLoader(\n",
    "    gluon.data.vision.ImageFolderDataset(train_path).transform_first(transform_train),\n",
    "    batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "test_dataset = gluon.data.DataLoader(\n",
    "    gluon.data.vision.ImageFolderDataset(train_path).transform_first(transform_train),\n",
    "    batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "\n",
    "# get pretrained squeezenet\n",
    "net = models.squeezenet1_1(pretrained=True, prefix='deep_crash_helmet_')\n",
    "# crash helemt happens to be a class in imagenet.\n",
    "# we can reuse the weight for that class for better performance\n",
    "# here's the index for that class for later use\n",
    "# See https://gist.github.com/aaronpolhamus/964a4411c0906315deb9f4a3723aac57\n",
    "imagenet_helmet_index = 778"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safety Helmet Net\n",
    "\n",
    "In vision networks it is common that the first set of layers learns the task of recognizing edges, curves and other important visual features of the input image. We call this feature extraction, and once the abstract features are extracted we can leverage a much simpler model to classify images using these features.\n",
    "\n",
    "We will use the feature extractor from the pretrained squeezenet (every layer except the last one) to build our own classifier for safety helmets. Conveniently, the MXNet model zoo handles the decapitation for us. All we have to do is specify the number of output classes in our new task, which we do via the keyword argument `classes=2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SqueezeNet(\n",
      "  (features): HybridSequential(\n",
      "    (0): Conv2D(3 -> 64, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (1): Activation(relu)\n",
      "    (2): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=True)\n",
      "    (3): HybridSequential(\n",
      "      (0): HybridSequential(\n",
      "        (0): Conv2D(64 -> 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Activation(relu)\n",
      "      )\n",
      "      (1): HybridConcurrent(\n",
      "        (0): HybridSequential(\n",
      "          (0): Conv2D(16 -> 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Activation(relu)\n",
      "        )\n",
      "        (1): HybridSequential(\n",
      "          (0): Conv2D(16 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): Activation(relu)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): HybridSequential(\n",
      "      (0): HybridSequential(\n",
      "        (0): Conv2D(128 -> 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Activation(relu)\n",
      "      )\n",
      "      (1): HybridConcurrent(\n",
      "        (0): HybridSequential(\n",
      "          (0): Conv2D(16 -> 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Activation(relu)\n",
      "        )\n",
      "        (1): HybridSequential(\n",
      "          (0): Conv2D(16 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): Activation(relu)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=True)\n",
      "    (6): HybridSequential(\n",
      "      (0): HybridSequential(\n",
      "        (0): Conv2D(128 -> 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Activation(relu)\n",
      "      )\n",
      "      (1): HybridConcurrent(\n",
      "        (0): HybridSequential(\n",
      "          (0): Conv2D(32 -> 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Activation(relu)\n",
      "        )\n",
      "        (1): HybridSequential(\n",
      "          (0): Conv2D(32 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): Activation(relu)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): HybridSequential(\n",
      "      (0): HybridSequential(\n",
      "        (0): Conv2D(256 -> 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Activation(relu)\n",
      "      )\n",
      "      (1): HybridConcurrent(\n",
      "        (0): HybridSequential(\n",
      "          (0): Conv2D(32 -> 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Activation(relu)\n",
      "        )\n",
      "        (1): HybridSequential(\n",
      "          (0): Conv2D(32 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): Activation(relu)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(0, 0), ceil_mode=True)\n",
      "    (9): HybridSequential(\n",
      "      (0): HybridSequential(\n",
      "        (0): Conv2D(256 -> 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Activation(relu)\n",
      "      )\n",
      "      (1): HybridConcurrent(\n",
      "        (0): HybridSequential(\n",
      "          (0): Conv2D(48 -> 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Activation(relu)\n",
      "        )\n",
      "        (1): HybridSequential(\n",
      "          (0): Conv2D(48 -> 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): Activation(relu)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): HybridSequential(\n",
      "      (0): HybridSequential(\n",
      "        (0): Conv2D(384 -> 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Activation(relu)\n",
      "      )\n",
      "      (1): HybridConcurrent(\n",
      "        (0): HybridSequential(\n",
      "          (0): Conv2D(48 -> 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Activation(relu)\n",
      "        )\n",
      "        (1): HybridSequential(\n",
      "          (0): Conv2D(48 -> 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): Activation(relu)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): HybridSequential(\n",
      "      (0): HybridSequential(\n",
      "        (0): Conv2D(384 -> 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Activation(relu)\n",
      "      )\n",
      "      (1): HybridConcurrent(\n",
      "        (0): HybridSequential(\n",
      "          (0): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Activation(relu)\n",
      "        )\n",
      "        (1): HybridSequential(\n",
      "          (0): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): Activation(relu)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): HybridSequential(\n",
      "      (0): HybridSequential(\n",
      "        (0): Conv2D(512 -> 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Activation(relu)\n",
      "      )\n",
      "      (1): HybridConcurrent(\n",
      "        (0): HybridSequential(\n",
      "          (0): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): Activation(relu)\n",
      "        )\n",
      "        (1): HybridSequential(\n",
      "          (0): Conv2D(64 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): Activation(relu)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): Dropout(p = 0.5)\n",
      "  )\n",
      "  (output): HybridSequential(\n",
      "    (0): Conv2D(None -> 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Activation(relu)\n",
      "    (2): AvgPool2D(size=(13, 13), stride=(13, 13), padding=(0, 0), ceil_mode=False)\n",
      "    (3): Flatten\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "deep_safety_helmet_net = models.squeezenet1_1(prefix='deep_safety_helmet_', classes=2)\n",
    "deep_safety_helmet_net.collect_params().initialize()\n",
    "deep_safety_helmet_net.features = net.features\n",
    "\n",
    "# Lets take a look at what this network looks like\n",
    "print(deep_safety_helmet_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network can already be used for prediction. However, since it hasn't been finetuned yet so the network performance could not be optimal.\n",
    "\n",
    "Let's test it out by defining a prediction function to feed a local image into the network and get the predicted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgba2rgb\n",
    "\n",
    "def classify_safety_helmet(net, url):\n",
    "    I = io.imread(url)\n",
    "    if I.shape[2] == 4:\n",
    "        I = rgba2rgb(I)\n",
    "    image = mx.nd.array(I).astype(np.uint8)\n",
    "    image = mx.image.resize_short(image, 256)\n",
    "    image, _ = mx.image.center_crop(image, (224, 224))\n",
    "    image = mx.image.color_normalize(image.astype(np.float32)/255,\n",
    "                                     mean=mx.nd.array([0.485, 0.456, 0.406]),\n",
    "                                     std=mx.nd.array([0.229, 0.224, 0.225]))\n",
    "    image = mx.nd.transpose(image.astype('float32'), (2,1,0))\n",
    "    image = mx.nd.expand_dims(image, axis=0)\n",
    "    out = mx.nd.SoftmaxActivation(net(image))\n",
    "    print('Probabilities are: '+str(out[0].asnumpy()))\n",
    "    result = np.argmax(out.asnumpy())\n",
    "    outstring = ['Not wearing safety helmet', 'Wearing safety helmet']\n",
    "    print(outstring[result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets download a safety helmet image and an image not wearing a helmet to our local directory to test this model on\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Wearing helmet](https://www.worksafe.qld.gov.au/__data/assets/image/0017/161333/ris-workplace-exposure-standards-framework-banner.jpg)\n",
    "\n",
    "![Not wearing helmet](./test/neg/996.jpg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities are: [ 0.4962869   0.50371313]\n",
      "Wearing safety helmet\n",
      "Probabilities are: [ 0.59094107  0.40905896]\n",
      "Not wearing safety helmet\n"
     ]
    }
   ],
   "source": [
    "# To make the defined network run quickly we usually hybridize it first. \n",
    "# This also allows us to serialize and export our model\n",
    "deep_safety_helmet_net.hybridize()\n",
    "\n",
    "# Let's run the classification on downloaded images to see what our model comes up with\n",
    "classify_safety_helmet(deep_safety_helmet_net, 'https://www.worksafe.qld.gov.au/__data/assets/image/0017/161333/ris-workplace-exposure-standards-framework-banner.jpg') # check for wearing safety helmet\n",
    "classify_safety_helmet(deep_safety_helmet_net, './test/neg/996.jpg') # check for not wearing safety helmet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_safety_helmet_net.export('safety_helmet_or_not_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are not great, so we can use a \"fine tuning\" process (see https://gluon.mxnet.io/chapter08_computer-vision/fine-tuning.html), where we retrained the model with images of wearing hot dogs and not hot dogs. We can then apply these new parameters to our model to make it even more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridSequential(\n",
      "  (0): Conv2D(512 -> 1000, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): Activation(relu)\n",
      "  (2): AvgPool2D(size=(13, 13), stride=(13, 13), padding=(0, 0), ceil_mode=False)\n",
      "  (3): Flatten\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# let's examine the output layer and find the last conv layer\n",
    "print(net.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the last conv layer is the second layer\n",
    "pretrained_conv_params = net.output[0].params\n",
    "\n",
    "# weights can then be found from the above parameter dict\n",
    "pretrained_weight_param = pretrained_conv_params.get('weight')\n",
    "pretrained_bias_param = pretrained_conv_params.get('bias')\n",
    "\n",
    "# next, we locate the right slice that we're interested in.\n",
    "helmet_w = mx.nd.split(pretrained_weight_param.data().as_in_context(mx.cpu()),\n",
    "                       1000, axis=0)[imagenet_helmet_index]\n",
    "helmet_b = mx.nd.split(pretrained_bias_param.data().as_in_context(mx.cpu()),\n",
    "                       1000, axis=0)[imagenet_helmet_index]\n",
    "\n",
    "# our classifier is for two classes. here, we reuse the helmet class weight,\n",
    "# and randomly initialize the 'not helmet' class.\n",
    "new_classifier_w = mx.nd.concat(mx.nd.random_normal(shape=helmet_w.shape, scale=0.02),\n",
    "                                helmet_w,\n",
    "                                dim=0)\n",
    "new_classifier_b = mx.nd.concat(mx.nd.random_normal(shape=helmet_b.shape, scale=0.02),\n",
    "                                helmet_b,\n",
    "                                dim=0)\n",
    "\n",
    "# finally, we initialize the parameter buffers and set the values.\n",
    "# since classifier is a HybridSequential/Sequential, the following\n",
    "# takes the zero-indexed 1-st layer of the classifier\n",
    "final_conv_layer_params = deep_safety_helmet_net.output[0].params\n",
    "final_conv_layer_params.get('weight').set_data(new_classifier_w)\n",
    "final_conv_layer_params.get('bias').set_data(new_classifier_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return metrics string representation\n",
    "def metric_str(names, accs):\n",
    "    return ', '.join(['%s=%f'%(name, acc) for name, acc in zip(names, accs)])\n",
    "metric = mx.metric.create(['acc', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet.gluon as gluon\n",
    "from mxnet.image import color_normalize\n",
    "\n",
    "def evaluate(net, data_iter, ctx):\n",
    "    #data_iter.reset()\n",
    "    for batch in data_iter:\n",
    "        data = color_normalize(batch.data[0]/255,\n",
    "                               mean=mx.nd.array([0.485, 0.456, 0.406]).reshape((1,3,1,1)),\n",
    "                               std=mx.nd.array([0.229, 0.224, 0.225]).reshape((1,3,1,1)))\n",
    "        data = gluon.utils.split_and_load(data, ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = []\n",
    "        for x in data:\n",
    "            outputs.append(net(x))\n",
    "        metric.update(label, outputs)\n",
    "    out = metric.get()\n",
    "    metric.reset()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-8c061b9434bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mcontexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgpus\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mdeep_safety_helmet_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep_safety_helmet_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-77-8c061b9434bd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_iter, val_iter, epochs, ctx)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbest_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mval_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[Initial] validation: %s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-821461cec2ea>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(net, data_iter, ctx)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#data_iter.reset()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         data = color_normalize(batch.data[0]/255,\n\u001b[0m\u001b[1;32m      8\u001b[0m                                \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.485\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.456\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.406\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                std=mx.nd.array([0.229, 0.224, 0.225]).reshape((1,3,1,1)))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "import mxnet.autograd as autograd\n",
    "\n",
    "def train(net, train_iter, val_iter, epochs, ctx):\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': learning_rate, 'wd': wd})\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    best_f1 = 0\n",
    "    val_names, val_accs = evaluate(net, val_iter, ctx)\n",
    "    logging.info('[Initial] validation: %s'%(metric_str(val_names, val_accs)))\n",
    "    for epoch in range(epochs):\n",
    "        tic = time.time()\n",
    "        train_iter.reset()\n",
    "        btic = time.time()\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            # the model zoo models expect normalized images\n",
    "            data = color_normalize(batch.data[0]/255,\n",
    "                                   mean=mx.nd.array([0.485, 0.456, 0.406]).reshape((1,3,1,1)),\n",
    "                                   std=mx.nd.array([0.229, 0.224, 0.225]).reshape((1,3,1,1)))\n",
    "            data = gluon.utils.split_and_load(data, ctx_list=ctx, batch_axis=0)\n",
    "            label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "            outputs = []\n",
    "            Ls = []\n",
    "            with autograd.record():\n",
    "                for x, y in zip(data, label):\n",
    "                    z = net(x)\n",
    "                    # rescale the loss based on class to counter the imbalance problem\n",
    "                    L = loss(z, y) * (1+y*positive_class_weight)/positive_class_weight\n",
    "                    # store the loss and do backward after we have done forward\n",
    "                    # on all GPUs for better speed on multiple GPUs.\n",
    "                    Ls.append(L)\n",
    "                    outputs.append(z)\n",
    "                for L in Ls:\n",
    "                    L.backward()\n",
    "            trainer.step(batch.data[0].shape[0])\n",
    "            metric.update(label, outputs)\n",
    "            if log_interval and not (i+1)%log_interval:\n",
    "                names, accs = metric.get()\n",
    "                logging.info('[Epoch %d Batch %d] speed: %f samples/s, training: %s'%(\n",
    "                               epoch, i, batch_size/(time.time()-btic), metric_str(names, accs)))\n",
    "            btic = time.time()\n",
    "\n",
    "        names, accs = metric.get()\n",
    "        metric.reset()\n",
    "        logging.info('[Epoch %d] training: %s'%(epoch, metric_str(names, accs)))\n",
    "        logging.info('[Epoch %d] time cost: %f'%(epoch, time.time()-tic))\n",
    "        val_names, val_accs = evaluate(net, val_iter, ctx)\n",
    "        logging.info('[Epoch %d] validation: %s'%(epoch, metric_str(val_names, val_accs)))\n",
    "\n",
    "        if val_accs[1] > best_f1:\n",
    "            best_f1 = val_accs[1]\n",
    "            logging.info('Best validation f1 found. Checkpointing...')\n",
    "            net.save_params('safety-helmet-%d.params'%(epoch))\n",
    "\n",
    "if mode == 'hybrid':\n",
    "    deep_safety_helmet_net.hybridize()\n",
    "if epochs > 0:\n",
    "    contexts = [mx.gpu(i) for i in range(gpus)] if gpus > 0 else [mx.cpu()]\n",
    "    deep_safety_helmet_net.collect_params().reset_ctx(contexts)\n",
    "    train(deep_safety_helmet_net, train_dataset, test_dataset, epochs, contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Use the new fine-tuned params "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions seem reasonable, so we can export this as a serialized model to our local directory. This is a simple one line command, which produces a set of two files: a json file holding the network architecture, and a params file holding the parameters the network learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's push this serialized model to S3, where we can then optimize it for our DeepLense device and then push it down onto our device for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): sts.amazonaws.com\n",
      "INFO:botocore.vendored.requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::622803848910:role/SageMaker_role_IM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "s3.Object(bucket_name='sagemaker-test1', key='hotdog_or_not_model-0000.params')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import re\n",
    "\n",
    "assumed_role = boto3.client('sts').get_caller_identity()['Arn']\n",
    "s3_access_role = re.sub(r'^(.+)sts::(\\d+):assumed-role/(.+?)/.*$', r'\\1iam::\\2:role/\\3', assumed_role)\n",
    "print(s3_access_role)\n",
    "s3 = boto3.resource('s3')\n",
    "bucket= 'your s3 bucket name here' \n",
    "\n",
    "json = open('hotdog_or_not_model-symbol.json', 'rb')\n",
    "params = open('hotdog_or_not_model-0000.params', 'rb')\n",
    "s3.Bucket(bucket).put_object(Key='test/hotdog_or_not_model-symbol.json', Body=json)\n",
    "s3.Bucket(bucket).put_object(Key='test/hotdog_or_not_model-0000.params', Body=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
